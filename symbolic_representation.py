# -*- coding: utf-8 -*-
"""Symbolic_Representation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sJy-UkFOA1eQeY4XTi2CO9YSNdR2jlgg
"""

import sympy as sp
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertModel
import numpy as np

# Task 1: Generate Histogram Dataset
def generate_histogram_dataset(n_samples=1000, bins=10):
    dataset = []
    x = sp.Symbol('x')
    functions = [sp.sin(x), sp.cos(x), sp.exp(x), sp.log(1+x), sp.tan(x)]
    for _ in range(n_samples):
        func = np.random.choice(functions)
        hist_values, bin_edges = np.histogram(np.random.rand(100), bins=bins)
        taylor_exp = sp.series(func, x, 0, 5).removeO()
        dataset.append((hist_values.tolist(), str(taylor_exp)))
    return dataset

dataset = generate_histogram_dataset()

# Tokenization
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
def tokenize_function(exp):
    return tokenizer(exp, padding="max_length", truncation=True, max_length=50, return_tensors="pt")

tokenized_data = [(torch.tensor(f, dtype=torch.float32), tokenize_function(t)) for f, t in dataset]

# Custom Dataset Class
class HistogramDataset(Dataset):
    def __init__(self, tokenized_data):
        self.data = tokenized_data
    def __len__(self):
        return len(self.data)
    def __getitem__(self, idx):
        histogram, tokenized_exp = self.data[idx]
        return histogram.float(), tokenized_exp['input_ids'].squeeze(0).long()

dataloader = DataLoader(HistogramDataset(tokenized_data), batch_size=16, shuffle=True)

# Task 2: LSTM Model
class LSTMModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)
    def forward(self, x):
        x = x.unsqueeze(1)
        lstm_out, _ = self.lstm(x)
        return self.fc(lstm_out[:, -1, :])

lstm_model = LSTMModel(input_dim=10, hidden_dim=256, output_dim=50)
criterion = nn.MSELoss()
optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)

# Training Loop for LSTM
for epoch in range(5):
    for inputs, targets in dataloader:
        optimizer.zero_grad()
        inputs = inputs.float()
        targets = targets.float()
        outputs = lstm_model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
    print(f"Epoch {epoch+1}, Loss: {loss.item()}")

# Task 3: Transformer Model
class TransformerModel(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(TransformerModel, self).__init__()
        self.transformer = BertModel.from_pretrained("bert-base-uncased")
        self.fc = nn.Linear(768, output_dim)
    def forward(self, x):
        outputs = self.transformer(input_ids=x)[0]
        return self.fc(outputs[:, 0, :])

transformer_model = TransformerModel(input_dim=768, output_dim=50)
criterion = nn.MSELoss()
optimizer = optim.Adam(transformer_model.parameters(), lr=0.001)

# Training Loop for Transformer
for epoch in range(5):
    for inputs, targets in dataloader:
        optimizer.zero_grad()
        inputs = inputs.long()
        targets = targets.float()
        outputs = transformer_model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
    print(f"Epoch {epoch+1}, Loss: {loss.item()}")